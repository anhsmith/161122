---
title: "Lecture C03"
subtitle: "A minimal theory of linear models"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "part_c.css"]
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---

```{r setup, echo=FALSE}
library(knitr)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(patchwork)
library(fontawesome)
opts_chunk$set(dev.args=list(bg='transparent'), comment="", warning=FALSE, echo=FALSE)
col_points <- "#7f577492"
col_dark   <- "#5f4354"
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="70%", fig.retina=2)
```

## Learning Outcomes

- Assumptions of the linear model.

- Least square estimation.

- Sampling distribution of least square estimator.

---

## Recap

- The general idea: find a (best possible) line to summarise the relationship between two variables, by finding values of the intercept and slope parameters.

- This is the job of the `lm()` function in R.

- We can summarise the results of a `lm()` object with `summary()`.

- We can plot the fitted line onto a the scatter plot of $x$ vs $y$ with `geom_abline()`.

---

## Focus

- When is a linear regression model appropriate? Sometimes, it is not clever to use a linear model, say predicting the numbers of confirmed cases of Covid-19 in NZ over time.

- How can we find the best intercept and the slope? What criterion does `lm()` use to choose them?

- What is the underlying model for linear regression? (L-I-N-E)

- What assumptions lead to the criterion that `lm()` uses?

---

class: middle, inverse

## The Gauss-Markov Assumptions

---

## Assumptions of the linear model: L-I-N-E

**L**inearity

 - $x$ and $y$ are linearly related. Residuals don't depend on $x$. 

**I**ndependence

 - Residuals don't influence each other.

**N**ormality

 - Residuals are distributed normally.

**E**qual variance

 - Residuals have constant variance. The variation doesn't change as we move along the trend.

---

## A mathematical formualtion of the linear model
$$
y=\alpha+\beta x +\varepsilon
$$

- Response variable: $y$ 

- Explanatory/predictor variable: $x$

- Regression coefficients (parameters): $\alpha$ and $\beta$

- Random error (residual): $\varepsilon$

---

## A mathematical formualtion of the linear model

Say, we have observed multiple pairs of $y$ and $x$ as $(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)$.

$$
y_1=\alpha+\beta x_1 +\varepsilon_1,
$$
$$
y_2=\alpha+\beta x_2 +\varepsilon_2,
$$
$$
\vdots
$$
$$
y_n=\alpha+\beta x_n +\varepsilon_n
$$

---

## Gauss-Markov Assumptions 

Given a linear model $y_i=\alpha+\beta x_i +\varepsilon_i,i=1,\ldots,n$, all four assumptions can be summarised in the following expression:

$$\varepsilon_i \mathop{\sim}\limits_\mathsf{iid} \mathsf{Normal}(0, \sigma^2)$$

- $\mathsf{mean}[\varepsilon_i]=0$ ensures the **L**inearity.

- $\mathsf{iid}$(*identically and independently distributed*) ensures the **I**ndependence.

- $\mathop{\sim}\mathsf{Normal}$ ensures the **N**ormality. 

- $Var(\varepsilon_i)=\sigma^2$ ensures the **E**qual variance

These four assumptions are called the "**the Guass-Markov assumptions**".

---

## Wny do we need assumptions? 

--

- Without assumptions, there can be no conclusions.

--

- These four assumptions are critical for deriving the theory of linear model.

--

- They are even more critical in practice. If the assumptions are not true and we go ahead and assume them anyway, **our statistical inferences can be wrong**.

--

- Some assumptions are more important than others - the seriousness of violating them varies.

--

- Lecture C4 - Residual Diagnostics will investigate this topic in more detail.

---

class: middle, inverse

## Least Sqaures Estimation

---

## Minimising the (squared) residuals

Supposing that the Gauss-Markov assumptions are all satisfied, we can now estimate the regression coefficients $\alpha$ and $\beta$. 

--

Denote a pair of arbitrary guesses by $\tilde{\alpha}$ and $\tilde{\beta}$. Without divine intervention, these won't exactly equal the true values $\alpha$ and $\beta$. **Why not?**

--

For each point $x_i$, guesses $\tilde{\alpha}$ and $\tilde{\beta}$, we have the corresponding estimated value $\hat{y}_i = \tilde\alpha + \tilde\beta x_i$.

--

The difference between the real $y_i$ and fitted $\hat{y}_i$ is the **residual** $\hat\varepsilon_i = y_i - \hat{y}_i$.

--

A good combination of $\tilde{\alpha}$ and $\tilde{\beta}$ minimises the residuals -- gets them close to zero. 

---

## Minimising the variance of residuals

--

If we have only two pairs of observations, we can easily draw a line between them with residuals zero. 

--

Given more then two points, what can we do ?

--

We need some tools to measure the overall performance of a fitted line in terms of residuals. 

--

One way is to *minimise the variance of the residuals*, subject to their mean being 0. (Why mean 0?)

---

## Least-squares estimation

--

This is called **least-squares estimation**, as the formula for variance contains a sum of squares
$$
\begin{aligned}
\mathsf{var}(\tilde\varepsilon_i) &= \frac{1}{n}\sum_{i=1}^n (\tilde\varepsilon_i - 0)^2= \frac{1}{n}\sum_{i=1}^n [y_i - (\tilde\alpha + \tilde\beta x_i)]^2
\end{aligned}
$$
--

By minimising the **residual variance** $\mathsf{Var}_\mathsf{res} = \frac{1}{n}\sum_{i=1}^n [y_i - (\hat\alpha + \hat\beta x_i)]^2$, we obtain the corresponding values of $\tilde\alpha$ and $\tilde\beta$ which are called **least squares estimates**.

--

We often put hats on parameters ( $\hat{\alpha}$, $\hat{\beta}$ ) to denote the least-squares estimates. This helps us to remember that they're sample statistics, differentiating them from the true values $\alpha$ and $\beta$ from the population and the arbitrary guesses $\tilde\alpha$ and $\tilde\beta$. 

--

In previous lectures we sometimes used $a$ and $b$ to talk about $\hat{\alpha}$ and $\hat{\beta}$.

---

## A toy example

--

Let's consider a even simpler model: $\mathsf{mean}[y]=\beta x$. The intercept $\alpha$ is set as zero. We only need to estimate $\beta.$

--

We observed two pairs of observations $(x_1,y_1)=(4,5)$ and $(x_2,y_2)=(6,3)$.

--

Then,

$$\mathsf{Var}_\mathsf{res}(\tilde\beta)=[(y_1-\tilde\beta x_1)^2+(y_2-\tilde\beta x_2)^2]/2$$

--

After some simplifications, we have 
 
$$\mathsf{Var}_\mathsf{res}(\tilde\beta)=a\tilde\beta^2+b\tilde\beta+c$$

where $a=(x_1^2+x_2^2)/2=26,b=-(x_1y_1+x_2y_2)=-38,c=(y_1^2+y_2^2)/2=17$.

--

This leads to $\mathsf{Var}_\mathsf{res}(\tilde\beta)=26\tilde\beta^2-38\tilde\beta+17$, a quadratic function of $\tilde\beta$. Let's plot the residual variance against possible values of $\tilde\beta$...

---

```{r, fig.align="center", fig.width=8, fig.height=3}
Var_res <- function(beta_tilde) 26*beta_tilde^2-38*beta_tilde+17 
ggplot() + xlim(0, 1.5) + ylim(0,20) +  geom_function(fun = Var_res, col='red') +
  geom_vline(xintercept = 19/26, col='blue', alpha=0.5) +
  geom_hline(yintercept = Var_res(19/26), col='black', alpha=0.4) +
  xlab(latex2exp::TeX("$\\tilde{\\beta}$")) + ylab("Variance of Residuals")
```

---

## A toy example

--

Some simple calculus reveals that $\mathsf{Var}_\mathsf{res}$ attains its minimum at $\tilde\beta=19/26\approx0.7308$.

--

This is an 'analytical solution'.

--

We can of course leave all the calculations to R. To fit a linear model without an intercept, we add `- 1` to the right hand side of the formula `y ~ x`.

```{r, echo=TRUE, eval=TRUE}
x <- c(4,6)
y <- c(5,3)
lm_no_intercept  <- lm( y ~ x - 1 )
coef(lm_no_intercept)
```

---

## Try some harder examples

--

Given three or more pairs of observations, we can still estimate $\beta$ manually or with R.

--

**Task**: Add a new observation $(x_3,y_3)=(9,12)$ to the two observations in our toy example. Re-estimate $\beta$ with all three observations with R.   

--

One can further estimate both the slope and the intercept in a linear model provided that the intercept is non-zero. To estimate two parameters simultaneously, i.e. $\alpha$ and $\beta$, we will need some **multivariate calculus**. 

--

More generally, the procedure of finding parameter values that minimise some loss function (e.g., variance of the residuals) is called **optimisation**. **Optimisation** plays a very important role in modern statistics!

--

Let's not dwell on the mathematical details. Conveniently, R does all the work for us!

---

## Estimators vs estimates

--

Returning to the toy example, this time using the abstract symbols $(x_1,y_1)$ and $(x_2,y_2)$. 

--

We can calculate the residual variance as

$$\mathsf{Var}_\mathsf{res}(\tilde\beta)=[(x_1^2+x_2^2)\tilde\beta^2-2(x_1y_1+x_2y_2)\tilde\beta+(y_1^2+y_2^2)]/2$$

--

The minimum of residual variance is attained at the least-squares estimate 
$$\hat\beta=\frac{x_1y_1+x_2y_2}{x_1^2+x_2^2}$$
--

This generic form of $\hat\beta$ is called '**the least-squares estimator**'. It is generic, because you can apply it to any set of paired values of $x$ and $y$. How easy is that?!

--

For example, if we substitute the numerical values of observed data into the estimator, we obtain $\hat\beta=19/26\approx0.7308$. $\hat\beta$ is a '**least-squares estimate**'. 

---

## Estimators vs estimates

--

An **estimate** is calculated from the real data but an **estimator** is calculated (derived) from the abstract data. An 'estimator' is a method. An 'estimate' is a result obtained from applying the method.

--

If you know the mathematical form of the least-squares estimator, you can easily obtain the least-squares estimates for a particular sample. 

--

The summary of `lm()` provides us the least-squares estimates, rather than the estimators. Actually, the least-squares estimators are packaged up in the `lm()` function so we can run it with any data set.

--

Statisticians spend a lot of effort to find good estimators and programming them into R packages!

---

## Estimator as a random variable

Recall that $y_1=\beta x_1+\varepsilon_1$ and $y_2=\beta x_2+\varepsilon_2$. Substituting the right hand side for $y$ in our least-squares estimator $\hat\beta=\frac{x_1y_1+x_2y_2}{x_1^2+x_2^2}$ yields

$$\hat\beta=\frac{x_1(\beta x_1+\varepsilon_1)+x_2(\beta x_2+\varepsilon_2)}{x_1^2+x_2^2}=\beta+\frac{x_1\varepsilon_1+x_2\varepsilon_2}{x_1^2+x_2^2}$$

--

$\hat\beta$ can be regarded as the sum of the ground truth $\beta$ and the random disturbance $\frac{x_1\varepsilon_1+x_2\varepsilon_2}{x_1^2+x_2^2}$.

--

Therefore, $\hat\beta$ is also a random variable! But what does this mean? Why does it vary, and over what?

--

The source of the randomness is called '*sampling variation*'. 

--

- If you were to take a different sample from the population, the observed $y$ values would be different. 

--

- With different $y$ values, you get different residuals $\varepsilon$ (deviations of individual $y$ values from their fitted values $\hat{y}$).

--

- Thus, the estimates of $\hat\beta$ vary across different sample datasets.

---


## Hints on the mean of random variables 

$$ 
\begin{aligned}
\mathsf{mean}[X+Y]&=\mathsf{mean}[X]+\mathsf{mean}[Y]\\
 \mathsf{mean}[X+Y+Z]&=\mathsf{mean}[X]+\mathsf{mean}[Y]+\mathsf{mean}[Z]\\
\mathsf{mean}[aX]&=a\cdot\mathsf{mean}[X]\\
 \mathsf{mean}[aX+b]&=a\cdot\mathsf{mean}[X]+b\\
\mathsf{mean}[aX+bY]&=???\\
  \mathsf{mean}\left[\sum_{i=1}^nX_i\right]&=???\\
    \mathsf{mean}\left[\sum_{i=1}^na_iX_i\right]&=???
\end{aligned}
$$

---


## Hints on the mean of random variables 

$$ 
\begin{aligned}
\mathsf{mean}[X+Y]&=\mathsf{mean}[X]+\mathsf{mean}[Y]\\
 \mathsf{mean}[X+Y+Z]&=\mathsf{mean}[X]+\mathsf{mean}[Y]+\mathsf{mean}[Z]\\
\mathsf{mean}[aX]&=a\cdot\mathsf{mean}[X]\\
 \mathsf{mean}[aX+b]&=a\cdot\mathsf{mean}[X]+b\\
\mathsf{mean}[aX+bY]&=a\cdot\mathsf{mean}[X]+b\cdot\mathsf{mean}[Y]\\
  \mathsf{mean}\left[\sum_{i=1}^nX_i\right]&=\sum_{i=1}^n\mathsf{mean}[X_i]\\
    \mathsf{mean}\left[\sum_{i=1}^na_iX_i\right]&=\sum_{i=1}^na_i\cdot\mathsf{mean}[X_i]
\end{aligned}
$$


---


## Sampling distribution of our estimators

--

Under the Guass-Markov assumptions, we know the distribution of $\hat\alpha$ and $\hat\beta$ across different samples.

--

It can be shown that $\mathsf{mean}[\hat\alpha]= \alpha$ and $\mathsf{mean}[\hat\beta]=\beta$. What does this mean?   

--

On average, across samples, the least-squares estimators get it right. The distribution of $\hat\alpha$ and $\hat\beta$ are centred on the true parameters $\alpha$ and $\beta$. In other words, they're unbiassed estimators!

--

We also know that the standard errors of $\hat\alpha$ and $\hat\beta$ decrease as the sample size $n$ gets larger. The bigger the sample, the closer the estimates get to their true values.

--

Knowledge of the distribution of estimates $\hat\alpha$ and $\hat\beta$ can be used to test hypotheses about the true values $\alpha$ and $\beta$.

---

class: middle, inverse

# Plot a simple function in R

---

## Function in R

To plot a function, must learn to write functions in R. 

We write a simple function to evaluate the residual variance in the toy example $\mathsf{Var}_\mathsf{res}(\tilde\beta)=26\tilde\beta^2-38\tilde\beta+17$.

```{r, echo=TRUE}
Var_res <- function(beta_tilde) 26*beta_tilde^2-38*beta_tilde+17
Var_res(2)
```

`Var_res()` is a user-defined function, i.e. our own R functions. 

It will be sufficient for us to write some simple R functions as statistician has written most complicated R functions for us!

---

```{r, echo=TRUE, fig.align="center", fig.width=6, fig.height=2.5}
Var_res <- function(beta_tilde) 26*beta_tilde^2-38*beta_tilde+17
ggplot() + xlim(0, 1.5) + ylim(0,20) + geom_function(fun = Var_res, col='red') +
  geom_vline(xintercept = 19/26, col='blue', alpha=0.5) +
  geom_hline(yintercept = Var_res(19/26), col='yellow', alpha=0.5) +
  xlab("beta_tilde") + ylab("Variance of Residuals")
```

---

## Hint on making the plot

- The key is `geom_function()`. But before calling it, you should create your ggplot object by `ggplot()` and specify the range of $x$ by `xlim()`. 

- In `geom_function()`, specify the function to be plotted in the argument `fun`. You can try some different functions, e.g. `exp`, `dnorm` and `pnorm`.

- We add the vertical line and horizontal line corresponding to the minimum of the residual variance by `geom_vline()` and `geom_vline()` by specifying the argument `xintercept` and `yintercept`.
This can also be done by `geom_abline()` with the same arguments. 

- Try different values in `xlim()` and `ylim()` and adjust `alpha` and `color` for better visualization effects.


---

## Summary

- Assumptions for linear model

- Least squares estimation

- Estimator, estimate, and sampling distribution

- Create your own functions and make its plot

