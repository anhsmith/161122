---
title: "Lecture C08"
subtitle: "Autoregression and Autocorrelation"
output:
  xaringan::moon_reader:
    css: [default, default-fonts, "part_c.css"]
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"
    nature:
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
editor_options:
  chunk_output_type: console
---


```{r setup, echo=FALSE, message=FALSE}
library(knitr)
library(forecast)
library(TSA)
library(tsibble)
library(ggplot2); theme_set(theme_bw(base_size=15))
library(visreg)
library(tidyverse)
library(RBNZ)
library(janitor)
library(patchwork)
library(latex2exp)
library(broom)
opts_chunk$set(#dev.args=list(bg='transparent'), 
               comment="", warning=FALSE, echo=FALSE)
knitr::opts_knit$set(root.dir = "../")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.dim=c(8,5), out.width="80%", fig.retina=2, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE)
```

```{r, echo=FALSE, message=FALSE}

# data downloaded from https://www.rbnz.govt.nz/statistics/series/economic-indicators/housing

read_rnzb <- function(file, skippy = 0) {
  require(readxl)
  require(janitor)
  read_xlsx(file, skip=5, 
              col_names = c("Date", read_xlsx(file, n_max=0, skip=skippy) %>% names ) ) %>%
  clean_names
}

housing <- read_rnzb("data/hm10.xlsx")

```


## Learning Outcomes

- Lag-plot

- Autoregression

- Autocorrelation

---

## Recap

- Time Series

- Trend, Cycle, Seasonality, Irregularity

---

class: middle, inverse

# Residuals of time series 

---

## NZ housing time series

```{r,echo=FALSE}

gh1 <- housing %>% 
  ggplot() + 
  aes(x=date, y=total_value_of_housing_stock) + 
  geom_hline(yintercept=0) + 
  geom_point() + geom_line() +
  xlab('Time') + ylab('Total value of NZ housing stock') +
  scale_y_continuous(labels = 
   scales::dollar_format(prefix="$", scale=1e-6, suffix = "T",accuracy=.5),
   limits = c(-120000,1800000) 
   )

gh1
```

.small[
https://www.rbnz.govt.nz/statistics/series/economic-indicators/housing
]

---

## Fitting a quadratic model

Here's a quadratic ('second-order polynomial') model. 

$$y_t = \alpha + \beta_1 t + \beta_2 t^2 + \varepsilon_t$$
where

$\ \ \ \ \ \ y_t$ is the total value of NZ housing stock at time $t=1,2,\ldots,T$ ,    
$\ \ \ \ \ \ \alpha$ is the intercept,    
$\ \ \ \ \ \ \beta_1$ is the linear trend through time,    
$\ \ \ \ \ \ \beta_2$ is the quadratic (curvy) trend through time, and     
$\ \ \ \ \ \ \varepsilon_t$ are the residuals. 


--

In `R`, polynomial models are best fit with the `poly(x,2)` syntax. 

```{r,echo=TRUE}
housing <- housing %>% mutate( time = 1:n() )
mod_h_q <- lm(total_value_of_housing_stock ~ poly(time,2), data=housing)

```

---

## Quadratic model of NZ housing time series

```{r echo=FALSE}
aug_h_q <- augment(mod_h_q, housing)

gh1 +
  geom_smooth(data = aug_h_q,
              method='lm', formula = y ~ poly(x,2), col=4) +
  geom_hline(yintercept=0) + 
  annotate(geom="text", x = housing$date[7], y = 270000, label="Quadratic fit", col=4) 
```


---

## Quadratic model of NZ housing time series with residuals

```{r echo=FALSE}


gh1 +
  geom_smooth(data = aug_h_q,
              method='lm', formula = y ~ poly(x,2), col=4) +
  geom_hline(yintercept=0) + 
  annotate(geom="text", x = housing$date[7], y = 270000, label="Quadratic fit", col=4) +
  geom_segment(data = aug_h_q,
               aes(x=date,
                   xend=date,
                   y=total_value_of_housing_stock,
                   yend=.fitted),
               col=6) +
  annotate(geom="text", x = housing$date[63], y = 230000, label="Residuals", col=6) + 
  annotate('curve', curvature = 0.2, col = 6,
           x=housing$date[69], xend = housing$date[69], 
           y=2.5e5, yend = 5.2e5, 
           arrow=arrow(angle=20, type='closed', length=unit(0.1, 'inches'))) 
```

---


## Quadratic model of NZ housing time series with residuals

```{r echo=FALSE}

gh1 +
  geom_smooth(data = aug_h_q,
              method='lm', formula = y ~ poly(x,2), col=4) +
  geom_hline(yintercept=0, col=4, lwd=1.2) + 
  geom_point(data = aug_h_q,
            aes(y=.resid), col=6, size=1.2) +
  geom_line(data = aug_h_q,
            aes(y=.resid), col=6, lwd=.9) +
  geom_segment(data = aug_h_q,
               aes(x=date,
                   xend=date,
                   y=0,
                   yend=.resid),
               col=6) +
  geom_segment(data = aug_h_q,
               aes(x=date,
                   xend=date,
                   y=total_value_of_housing_stock,
                   yend=.fitted),
               col=6) +
  annotate(geom="text", x = housing$date[7], y = 270000, label="Quadratic fit", col=4) +
  annotate(geom="text", x = housing$date[63], y = 230000, label="Residuals", col=6) + 
  annotate('curve', curvature = 0.2, col = 6,
           x=housing$date[69], xend = housing$date[69], 
           y=2.5e5, yend = 5.2e5, 
           arrow=arrow(angle=20, type='closed', length=unit(0.1, 'inches'))) + 
  annotate('curve', curvature = 0.2, col = 6,
           x=housing$date[56]+4e6, xend = housing$date[56], 
           y=2e5, yend = 5e4, 
           arrow=arrow(angle=20, type='closed', length=unit(0.1, 'inches'))) 
```

---

## NZ housing time series with residuals

```{r echo=FALSE}

gh1 +
  geom_smooth(data = aug_h_q,
              method='lm', formula = y ~ poly(x,2), col=4) +
  geom_hline(yintercept=0, col=4, lwd=1.2) + 
  geom_point(data = aug_h_q,
            aes(y=.resid), col=6, size=1.2) +
  geom_line(data = aug_h_q,
            aes(y=.resid), col=6, lwd=.9) +
  annotate(geom="text", x = housing$date[7], y = 270000, label="Quadratic fit", col=4) +
  annotate(geom="text", x = housing$date[80], y = 230000, label="Residuals (observed - predicted)", col=6)
```

---

## Residual diagnostics for housing time series

```{r,echo=FALSE,fig.height=5.8}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(mod_h_q)
```

---

## Plot of residuals versus time 

```{r,echo=FALSE, fig.dim=c(8,5)}

ghr1 <- aug_h_q %>%
  ggplot() + 
  aes(x=date,y=.resid) + 
  geom_hline(yintercept=0, col=4, lwd=1.2) + 
  xlab('Time') + ylab('Residuals from quadratic model') + 
  ggtitle('Residuals: independent or cyclical?') +
  scale_y_continuous(labels = 
                       scales::dollar_format(prefix="$", scale=1/1e6, 
                                             suffix = "T",accuracy=.1),
                     limits = c(-150000,2.5e5) ) +
  geom_point(col=6, size=1.2) +
  geom_line(col=6, lwd=.5) +
  geom_smooth(col=7) +
  annotate(geom="text", x = housing$date[70], y = 170000, label="Residuals", col=6) +
  annotate(geom="text", x = housing$date[45], y = 60000, label="Smoother", col=7)

ghr1
```

---

## Plot of residuals versus time 

--

The lack of independence in the residuals in the previous plot is clear.

--

Recall that residuals in linear models ideally follow a normal distribution with zero mean, constant variance, and should vary independently. Basically, they should be a complete mess!

--

Let's compare the actual residuals to what they should look like...

---

## Comparing the residuals to what our model assumes them to be... oops

```{r,echo=FALSE}
set.seed(2020)

ghr2 <- ghr1 + xlab('Time') + ylab('Residuals') + 
  ggtitle('Actual residuals (what they are)')  +
  scale_y_continuous(limits = c(-250000,250000), 
                     labels = 
                       scales::dollar_format(prefix="$", scale=1/1e6, 
                                             suffix = "T",accuracy=.1)) + 
  theme(plot.title = element_text(size = 14))

ghrs1 <- aug_h_q %>%
  mutate(e=rnorm(n(), sd = mod_h_q %>% sigma))  %>% 
  ggplot(aes(x=date,y=e))  +  
  geom_hline(yintercept=0, col=4, lwd=1.2) +  
  xlab('Time') + ylab('Residuals')  + 
  ggtitle('Simulated residuals (what they should be)')+
  theme(plot.title = element_text(size = 14)) +
  scale_y_continuous(limits = c(-250000,250000), 
                     labels = 
                       scales::dollar_format(prefix="$", scale=1/1e6, 
                                             suffix = "T",accuracy=.1)) + 
  geom_point(col=6, size=1.2) +
  geom_line(col=6, lwd=.5) +
  geom_smooth(col=7) 

```


.pull-left[
```{r,echo=FALSE, fig.dim = c(6,4), out.width="120%"}
ghr2
```

- Multi-year cycles of consistently positive or consistently negative residuals     
- Most residuals are very close to their immediate neighbours 
]

--

.pull-right[
```{r,echo=FALSE, fig.dim = c(6,4), out.width="120%"}
ghrs1
```

- random fluctuation     
- no discernible pattern 
]

---

## Bad residuals versus good residuals

--

It is common for time series to have dependence structures. We can spot them by looking for patterns in plots of residuals, and there are tools we can use to measure them.

--

But why are non-independent residuals problematic? 

--

- We fit linear models to estimate parameters (e.g., linear change over time).    

--

- Importantly, linear models give us **measures of uncertainty** for our estimates. These measure the strength of evidence for a particular effect, and allow us to make conclusions and predictions.    

--

- For the measures of uncertainty to be accurate, certain assumptions must be met. One of these assumptions is independence of residuals.    

--

- If this assumption is not met, **we cannot trust our measures of uncertainty, and therefore our conclusions and predictions**.     


---

## Bad residuals versus good residuals

It is common for time series to have dependence structures. We can spot them by looking for patterns in plots of residuals, and there are tools we can use to measure them.

But why are non-independent residuals problematic? 

--

- Data are the currency of information. The more data we have, the more evidence we have. This is why 'degrees of freedom' are important.     

--

- Independent residuals imply that each data point represents an independent piece of information.     

--

- If the data points are not independent (conditional on the model; i.e. residuals), we have **less information** than data points.     

--

- If we assume the residuals are independent when, in reality, they're not, we're pretending we have more information than we actually have.

--

- **We therefore risk overstating the strength of our evidence and conclusions and the accuracy of our predictions**, if we make assumptions that are not true.

--

On the other hand, if the model adequately accounts for the dependence structures, the residuals will be independent, and we can trust our conclusions. 

--

Options to improve the model include incorporating more useful predictors in a linear model, or explicitly model the dependence structures (e.g., Box-Jenkins's autoregression).

---

## Bad residuals versus good residuals

.pull-left[
```{r,echo=FALSE, fig.dim = c(6,4), out.width="120%"}
ghr2 + ggtitle("Actual residuals")
```

- Multi-year cycles of consistently positive or consistently negative residuals     
- Most residuals are very close to their immediate neighbours 
]


.pull-right[
```{r,echo=FALSE, fig.dim = c(6,4), out.width="120%"}
ghrs1 + ggtitle("Simulated 'ideal' residuals")
```

- random fluctuation     
- no discernible pattern 
]

--

.center[
**'Lag plots' are a useful tool for examining and diagnosing dependence patterns.**
]


---

class: middle, inverse

# Lag plots

---

## Lag plots

A **lag** is a displacement of a certain amount of time; for example, one day, two hours.

Given a data set $y_1, y_2,\ldots,y_T$,     
$\ \ \ \ \ \ y_2$ and $y_7$ have lag 5 since $7 - 2 = 5$. 

A **lag plot** of lag $k$ is a plot of the values of $y_t$ versus $y_{t-k}$

  - Vertical axis: $\ \ \ \ \ \ \ Y_t$ for all $t=1,2,\ldots,T-k$
  - Horizontal axis: $\ \ Y_{t-k}$ for all $t=k+1,k+2,\ldots,T$

Lag plots can be generated for any arbitrary lag, although the most commonly used lag is 1.

---

## How to get the lagged values 

We can use `zlag()` function from `TSA` package. The argument `d=1` specifies a lag of one,  and it can be changed to another integer.

```{r,echo=TRUE}
library(TSA)
tibble(x=5:10) %>% mutate(x_lag1=zlag(x,d=1),
                          x_lag2=zlag(x,d=2))
```

An `NA` value is generated as there is no lag-one observation for the first `x`.

---

## Lag plot

The lag plot is an important visualisation tool for time series data. It can reveal unusual patterns in a data set, or time series, or residuals.

Independent and identically distributed data should not exhibit any identifiable structure in the lag plot for any lag.

Any discernible structure in the lag plot may indicate non-randomness or dependence structures in the data. 


---

## Lag one plots of residuals: good vs bad

.pull-left[

```{r,echo=FALSE,message=FALSE, out.width="110%"}
set.seed(2020)
aug_h_q %>% 
  mutate(.std.resid.lag1=zlag(.std.resid))%>%  
  ggplot(aes(x=.std.resid.lag1,y=.std.resid)) + 
  geom_hline(yintercept=0) + 
  geom_vline(xintercept=0) + 
  geom_line(lwd=1.2) + 
  geom_smooth(col=7) +
  xlab( TeX(r'(Previous residual $\epsilon_{t-1}$)') ) + 
  ylab( TeX(r'(Residual $\epsilon_{t}$)') ) + 
  ggtitle('Residuals vs lag-one residuals (actual data)')

```

- Very strong relationship between the residual at time $t$ vs residual at time $t-1$.

- Residuals never move much in one time step!

]

--


.pull-right[
```{r,echo=FALSE,message=FALSE, out.width="110%"}
aug_h_q %>% 
  mutate(e=rnorm(n())) %>% 
  mutate(e.lag1=zlag(e)) %>% 
  ggplot(aes(x=e.lag1,y=e))  +  
  geom_hline(yintercept=0) + 
  geom_vline(xintercept=0) + 
  geom_line() + 
  geom_smooth(col=7) +
  xlab( TeX(r'(Previous residual $\epsilon_{t-1}$)') ) + 
  ylab( TeX(r'(Residual $\epsilon_{t}$)') ) + 
  ggtitle('Residuals vs lag-one residuals (ideal simulated data)')

```

- No relationship between the residual at time $t$ vs residual at time $t-1$.

- Residuals separated by one time step completely unrelated!

]

---

## Lag two plots of residuals


.pull-left[

```{r,echo=FALSE,message=FALSE, out.width="110%"}
set.seed(2020)
aug_h_q %>% 
  mutate( .std.resid.lag2 = zlag(.std.resid, d=2) ) %>%  
  ggplot(aes(x = .std.resid.lag2,
             y = .std.resid) ) + 
  geom_hline(yintercept=0) + 
  geom_vline(xintercept=0) + 
  geom_line(lwd=1.2) + 
  geom_smooth(col=7) +
  xlab( TeX(r'(Previous previous residual $\epsilon_{t-2}$)') ) + 
  ylab( TeX(r'(Residual $\epsilon_{t}$)') ) + 
  ggtitle('Residuals vs lag-two residuals (actual data)')

```

- Still a strong relationship between the residual at time $t$ vs residual at time $t-2$.

- Residuals don't move much in two time steps!

]

--


.pull-right[
```{r,echo=FALSE,message=FALSE, out.width="110%"}
aug_h_q %>% 
  mutate(e=rnorm(n())) %>% 
  mutate(e.lag2=zlag(e,d=2)) %>% 
  ggplot(aes(x=e.lag2,y=e))  +  
  geom_hline(yintercept=0) + 
  geom_vline(xintercept=0) + 
  geom_line() + 
  geom_smooth(col=7) +
  xlab( TeX(r'(Previous previous residual $\epsilon_{t-2}$)') ) + 
  ylab( TeX(r'(Residual $\epsilon_{t}$)') ) + 
  ggtitle('Residuals vs lag-two residuals (ideal simulated data)')

```

- No relationship between the residual at time $t$ vs residual at time $t-2$.

- Residuals separated by two time steps completely unrelated!

]

---

## Model the linear trend in residuals


.left-code[
.small[
```{r echo=TRUE,eval=FALSE}
housing_resid_lag <- 
  aug_h_q %>% 
  mutate(
    .resid.lag1 = zlag(.resid, d=1)
    )

lm_r_lag1 <- lm(
  .resid ~ .resid.lag1, 
  data = housing_resid_lag
  )

summary(lm_resid_lag1)

```
]]

.right-plot[
.small[
```{r echo=FALSE}
housing_resid_lag <- 
  aug_h_q %>% 
  mutate(.resid.lag1 = zlag(.resid, d=1) )

lm_r_lag1 <- lm(.resid ~ .resid.lag1, 
                data = housing_resid_lag)

summary(lm_r_lag1) 

```
]]

--

.left-code-wide[

The fitted residuals have a linear trend with a slope of 1 (approximately). 
]


---

class: middle, inverse

# Autoregression

---

## Autoregression

Our (second order polynomial) model so far:

$$y_t = \alpha+\beta_1 t + \beta_2 t^2 + \varepsilon_t$$

where    
$\ \ \ \ y_t$ is the total value of NZ houses at time $t \in \{1,2,\ldots,T\}$,     
$\ \ \ \ \alpha$ is the intercept,    
$\ \ \ \ \beta_1$ is the linear effect of time,      
$\ \ \ \ \beta_2$ is the curvy effect of time, and    
$\ \ \ \ \varepsilon_t$ are the residuals from the model.

<br>

--

**Now, we will treat the fitted residuals** $\varepsilon_t$ **as a time series in and of itself.**

--

We have seen that each residual $\varepsilon_t$ is strongly related to the previous $\varepsilon_{t-1}$. 

--

Well, let's explicitly model the residuals with the previous residuals as a predictor!


---

## The AutoRegressive model

Now there are two layers to our model:

$$\begin{align}
y_t &= \alpha+\beta_1 t + \beta_2 t^2 + \varepsilon_t \ \ \ (1)\\
\varepsilon_t &= \phi \varepsilon_{t-1} + \epsilon_t  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2)
\end{align}$$

where       
$\ \ \ \ \alpha+\beta_1 t + \beta_2 t^2$ is a quadratic model of the broader trend,   
$\ \ \ \ \varepsilon_t$ are the residuals from the quadratic model,        
$\ \ \ \ \phi$ is the effect of one residual on the next residual, and       
$\ \ \ \ \epsilon_t$ are the residuals from the model of the residuals (phew!).


--

Each observation is a function of **(1) the broader trend** and **(2) a random walk** around or below that trend.

--

This is an **AutoRegressive** model with order (lag) 1, also known as an "**AR(1)**" model. 

--

Of course, model part (1) doesn't have to be quadratic. We can reduce it to a linear model by removing the $\beta_2 t^2$ term, if that fits the data better.

Here, we will use an even simpler model for $y_t$, the fitted residuals of total value of NZ housing market.

---

## A simpler time-series-difference model

As we already noticed, the slope of the residuals given the lag-one residuals is approximately 1, we can directly fix $\phi=1$.

In this case, the model is:
$$\varepsilon_t=\varepsilon_{t-1}+\epsilon_t$$

The advantage of this model is that the residuals of residuals can be easily obtained as
$$\epsilon_t=\varepsilon_t-\varepsilon_{t-1}$$

$\varepsilon_t-\varepsilon_{t-1}$ defines another time series model called **time series difference**. It can be interpreted as the incremental shifts from one observation to the next.

---

## Integrating time series differences 

For each time $t$, the incremental changes in the residuals are given by:
$$
\begin{aligned}
\epsilon_t&=\varepsilon_t-\varepsilon_{t-1}\\
\epsilon_{t-1}&=\varepsilon_{t-1}-\varepsilon_{T-2}\\
\epsilon_{t-2}&=\varepsilon_{t-2}-\varepsilon_{T-3}\\
~~&~~\vdots\\
\epsilon_2&=\varepsilon_{2}-\varepsilon_{1}\\
\epsilon_1&=\varepsilon_1-\varepsilon_0
\end{aligned}
$$

--

Summing over both LHS and RHS of the above equations gives
$$\epsilon_t+\epsilon_{t-1}+\cdots+\epsilon_2+\epsilon_1=\varepsilon_t-\varepsilon_0$$

---

## Integrating time series differences 

Letting $\varepsilon_0=0$, we have $\varepsilon_t=\sum_{i=1}^{t}\epsilon_i$ .

That is, the residual at time $t$ is just the sum of all the residual-residuals, the little random increments in the series so far. 

--

You can think of this as a **random walk** process. At time $t$, $\varepsilon_{t-1}$ walks to $\varepsilon_{t}$ with a random step of size $\epsilon_t$. 

--

The time series $y_t$ of total value of NZ housing market can then be written as
$$y_t=\alpha+\beta_1 t + \beta_2 t^2+\sum_{i=1}^{t}\epsilon_i$$
--

The model says the total value has a quadratic trend $\alpha+\beta_1 t + \beta_2 t^2$ and a random walk cycle $\sum_{i=1}^{t}\epsilon_i$.

--

The original residuals $\varepsilon_t$ are badly behaved, but the residual-residuals $\epsilon_t$ will be better!


---

## A crack in lag plot and autoregression

If the data has a deterministic trend without any randomness, it can have misleading lag plots. The following simulated data will illustrate this issue.

We are simulating `y` as a quadratic function of `time`, and adding the `lag1` version of `y` (`y_lag1`).


.small[
```{r}
simy <- tibble(time = 1:20) %>% 
  mutate(y = time^2) %>% 
  mutate(y_lag1 = zlag(y)) 
t(simy)
```
]


---

## A crack in lag plot and autoregression

.small[
```{r,echo=TRUE,fig.dim=c(12,4),fig.width="100%"}
g1 <- simy %>% ggplot(aes(x=time,   y=y       )) +geom_point()+ggtitle('Run chart')
g2 <- simy %>% ggplot(aes(x=y_lag1, y=y       )) +geom_point()+ggtitle('Lag 1 plot')
g3 <- simy %>% ggplot(aes(x=time,   y=y-y_lag1)) +geom_point()+ggtitle('Differenced run chart')
g1+g2+g3
```
]

---

## A crack in lag plot and autoregression

The simulated data is a quadratic trend as $y_t=t^2$.

We can get the corresponding differences between one value and the next as: $$y_t-y_{t-1}=t^2-(t-1)^2=2t-1$$

--

'Differencing' can make a quadratic trend look like a linear trend. So, you might end up fitting a linear-plus-difference model rather than a quadratic model.

--

It pays to try to model the overall trend appropriately before messing with the residuals.

---

class: middle, inverse

# Autocorrelation 

---

## Autocorrelation

--

The strong linear relationship between $Y_{t-1}$ and $Y_{t}$ can be examined by calculating the lag-one correlation.

--

Notice that the sample correlation coefficient is defined for a sequence of paired observations as $(x_1,y_1),\ldots,(x_n,y_n)$.

--

In time series, we have the pairs as

$$(y_1,y_2),(y_2,y_3),\ldots,(y_{t-1},y_{t}),(y_{t},y_{t+1}),\ldots,(y_{T-1},y_T)$$

--

The correlation between adjacent values in a series is called **autocorrelation** of order 1.

--

We can also calculate higher order **autocorrelation**, in steps of $2,3,4, \ldots$
$$(y_1,y_3),(y_2,y_4),\ldots,(y_{t-2},y_{t}),(y_{t-1},y_{t+1}),(y_{t},y_{t+2}),\ldots,(y_{T-2},y_T)$$

---

## Computing the autocorrelation

We can compute the autocorrelation coefficient easily by using `summarise()` and `cor()`.

```{r}
housing_resid_lag %>% 
  mutate(.resid.lag2 = zlag(.resid,d=2)) %>%
  summarise(AC1=cor(.resid, .resid.lag1, use = 'na.or.complete'),
            AC2=cor(.resid, .resid.lag2, use = 'na.or.complete'))
```

The argument `use = 'na.or.complete'` in `cor()` ensures that the `NA` values in the lagged series won't affect the computation of autocorrelation coefficients.

---

## Auto-Correlation Function (ACF)

We can examine autocorrelation coefficients across a range orders $k=0,1,2,\ldots$ , which is known as the **Auto-Correlation Function (ACF)**.

The ACF can be plotted in a so-called a '**correlogram**', obtainable with the `ggAcf()` function in  `forecast` package.


---

## Auto-Correlation Function (ACF) and Correlograms

--

.pull-left[

#### Correlogram of housing data residuals
.small[
```{r}
housing_resid_lag %>% 
  select(.resid) %>% ggAcf() 
```
]]

--

.pull-right[
#### Correlogram of independent residuals
.small[
```{r}
housing_resid_lag %>% 
  transmute(e=rnorm(n())) %>% ggAcf()
```
]]

--

.center[
Any values outside the blue dashed lines may indicate some autocorrelation. 
]

---

class: middle, inverse

# An Illustrative Example

---

## Air passenger data

The data set `AirPassengers` from the package `TSA` contains monthly totals of international airline passengers, 1949 to 1960. 

This might be the oldest data set in this paper.
.small[
```{r}
data(AirPassengers)
AirPassengers
```
]

---

## Data preprocessing

--

Turn the data to a tidy 'tsibble' (time series version of tibble) with the function `as_tsibble()` from the package `tsibble`.

--

Add the time index and the month index as a factor with `month()` from `lubridate`.

.small[
```{r}
library(tsibble); library(lubridate)
AP <- AirPassengers %>% as_tsibble() %>% mutate(time = 1:n(), month = factor(month(index))) 
AP
```
]

---

## Model 1: Simple linear regression
.small[
```{r}
lm1 <- lm(value ~ time, data=AP); summary(lm1)
```
]

---

## Model 1: Simple linear regression - confidence interval

.left-plot[
.small[
.middle[
```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}
gAP <- AP %>% ggplot() + aes(x=index, y = value) + geom_point() +
  xlab("Time") + ylab("Number of international passengers") + ylim(0,700)
gAP + geom_smooth(method = lm, alpha = .3, fill = 12)
```
]]]

--

.right-code[
.middle[
Looks reasonably linear.

*Any patterns in the residuals?*

]]


---

## Model 1: Simple linear regression - confidence and prediction intervals

.left-plot[
.small[
.middle[
```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}
gAP + geom_smooth(method = lm, alpha = .3, fill = 12) + geom_line()
```
]]]


.right-code[
.middle[
Looks reasonably linear.

*Any patterns in the residuals?*

A line reveals a pattern! 

*Is the residual variance constant?*

]]


---

## Model 1: Simple linear regression - confidence and prediction intervals

.left-plot[
.small[
.middle[
```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}

aug1_pi <- augment(lm1, AP, interval="prediction") 

gAP + 
  geom_smooth(method = lm, alpha = .3, fill = 12) +
  geom_ribbon(data = aug1_pi,
              aes(ymin = .lower, 
                  ymax = .upper), 
              alpha = .2) 
```
]]]


.right-code[
.middle[
Looks reasonably linear.

*Any patterns in the residuals?*

A line reveals a pattern!

*Is the residual variance constant?*

The grey ribbon gives the 95% *prediction* interval for the data. 

The model says the data should be scattered vertically around the line according to a normal distribution that is *identical* across the range of the x-axis. 

Does that look right?

]]

---

## Model 1: Simple linear regression - residual diagnostics
.left-plot[
```{r echo=FALSE, fig.dim=c(6,4.9), out.width="100%"}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(lm1)
```
]

--

.right.code[
Definitely an increase in residual variance!

And some curvature.
]

---

## Model 2: log-transformed $y$ (to stablise variablity)
.small[
```{r}
lm2_log = lm(log(value) ~ time, data=AP)
summary(lm2_log)
```
]
---

## Model 2: log-transformed $y$ - **confidence** interval

.left-code[
.small[
```{r eval=TRUE, echo=TRUE, fig.show="hide"}
aug2_log_ci <- 
  augment(lm2_log, AP, 
          interval="confidence")

g2_log_ci <- 
  ggplot(aug2_log_ci) + 
  aes(x = index, y = value, 
      ymin = exp(.lower), 
      ymax = exp(.upper)) + 
  geom_point() + 
  geom_ribbon(alpha = .2, fill = 12) + 
  geom_line(aes(y = exp(.fitted)), 
            col=4, lwd=1.2) +
  xlab("Time") + ylim(0,700) +
  ylab("International passengers") 

g2_log_ci
```
]]

.right-plot[
```{r , eval= TRUE, echo=FALSE, fig.dim=c(6,4), out.width="100%"}
g2_log_ci
```

]

---

## Model 2: log-transformed $y$ - **confidence** and **prediction** intervals

.left-code[
.small[
```{r eval=TRUE, echo=TRUE, fig.show="hide"}
aug2_log_ci <- 
  augment(lm2_log, AP, 
          interval="confidence")

g2_log_ci <- 
  ggplot(aug2_log_ci) + 
  aes(x = index, y = value, 
      ymin = exp(.lower), 
      ymax = exp(.upper)) + 
  geom_point() + 
  geom_ribbon(alpha = .2, fill = 12) + 
  geom_line(aes(y = exp(.fitted)), 
            col=4, lwd=1.2) +
  xlab("Time") + ylim(0,700) +
  ylab("International passengers") 

aug2_log_pi <- 
  augment(lm2_log, AP, 
          interval="prediction")

g2_log_ci + geom_ribbon(
      data = aug2_log_pi, 
      alpha = .3 ) 
```
]]

.right-plot[
```{r , eval= TRUE, echo=FALSE, fig.dim=c(6,4), out.width="100%"}
g2_log_ci + geom_ribbon(
      data = aug2_log_pi, 
      alpha = .3 ) 

```


Better?
]

---

## Model 2: log-transformed $y$ - residual diagnostics
.left-plot[
```{r echo=FALSE, fig.dim=c(6,4.9), out.width="100%"}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(lm2_log)
```
]

---

## Comparison of models 1 and 2

.pull-left[
.center[
#### Model 1: `y ~ x`

```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}
gAP + 
  geom_smooth(method = lm, alpha = .3, fill = 12) +
  ylim(0,700) +
  geom_ribbon(data = aug1_pi,
              aes(ymin = .lower, 
                  ymax = .upper), 
              alpha = .2) 
```
]]

--

.pull-right[
.center[
#### Model 2: `log(y) ~ x`

```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}
g2_log_ci + 
  ylim(0,700) + 
  geom_ribbon(
      data = aug2_log_pi, 
      alpha = .2 ) 
```
]]


---

## Comparison of models 1 and 2

.pull-left[
.center[
#### Model 1: `y ~ x`

```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}
gAP + 
  geom_smooth(method = lm, alpha = .3, fill = 12) +
  ylim(0,700) +
  geom_ribbon(data = aug1_pi,
              aes(ymin = .lower, 
                  ymax = .upper), 
              alpha = .2) +
  geom_line()
```
]]

.pull-right[
.center[
#### Model 2: `log(y) ~ x`

```{r echo=FALSE, fig.dim=c(6,5), out.width="100%"}
g2_log_ci + 
  ylim(0,700) + geom_ribbon(
      data = aug2_log_pi, 
      alpha = .2 ) +
  geom_line()
```
]]


--

.center[
Yes, the model of log $y$ is a better fit but we still haven't dealt with the residual correlation!
]

---

## Model 2: log-transformed $y$ - correlogram of residuals
.left-plot[
.small[
```{r fig.dim=c(8,6)}
ggAcf(aug2_log_ci$.std.resid) + xlab("Lag (months)")
```
]]

--

.right-code[
A strange pattern.

Autocorrelation is:      
- *positive* at lag≈1 month    
- *negative* at lag≈6 months
- *positive* at lag≈12 months
- *negative* at lag≈18 months

Thoughts?
]


---

## Model 2: log-transformed $y$ - correlogram of residuals
.left-plot[
.small[
```{r fig.dim=c(8,6)}
ggAcf(aug2_log_ci$.std.resid) + xlab("Lag (months)")
```
]]

.right-code[
A strange pattern.

Autocorrelation is:      
- *positive* at lag≈1 month    
- *negative* at lag≈6 months
- *positive* at lag≈12 months
- *negative* at lag≈18 months


Thoughts?

This looks like seasonality!

]

---

## Model 3: log $y$ with seasonality
.small[

```{r eval = TRUE}
lm3_log_s <- lm(log(value) ~ time + month, data=AP)
summary(lm3_log_s) %>% coef %>% round(.,3)
```

]

---

## Model 3: log $y$ with seasonality - **confidence** and **prediction** intervals

.left-code[
.small[
```{r eval=TRUE, echo=TRUE, fig.show="hide"}
aug3_log_s_ci <- 
  augment(lm3_log_s, AP, 
          interval="confidence")

aug3_log_s_pi <- 
  augment(lm3_log_s, AP, 
          interval="prediction")

g3_log_s_ci_pi <- 
  ggplot(aug3_log_s_ci) + 
  aes(x = index, y = value, 
      ymin = exp(.lower), 
      ymax = exp(.upper)) + 
  geom_ribbon(data = aug3_log_s_pi, 
      alpha = .3 ) +
  geom_ribbon(alpha = .2, fill = 12) + 
  geom_line(aes(y = exp(.fitted)), 
            col=4, lwd=1.2) +
  geom_point() + 
  xlab("Time") + 
  ylab("International passengers") 


g3_log_s_ci_pi
```
]]

.right-plot[
```{r , eval= TRUE, echo=FALSE, fig.dim=c(6,4), out.width="100%"}
g3_log_s_ci_pi
```
]

--

Seasonality (by adding `month` as a predictor) has mopped up a lot of previously unmodelled variation.

---

## Model 3: log $y$ with seasonality - residual diagnostics

.left-plot[
```{r echo=FALSE, fig.dim=c(6,4.9), out.width="100%"}
par(mfrow=c(2,2), mar=c(4,4,2,2))
plot(lm3_log_s)
```
]

--

.right-code[

Residuals are still a bit funky.

]
---

## Model 3: log $y$ with seasonality - residuals versus time

.left-plot[
```{r echo=FALSE, fig.dim=c(6,4.9), out.width="100%"}
augment(lm3_log_s, AP) %>% 
  ggplot(aes(x=index, y=.std.resid)) + 
  geom_hline(yintercept = 0, col=4) + xlab("Time") +
  geom_point()+ geom_line() + geom_smooth(colour=7) 
```
]

.right-code[

Residuals are still a bit funky.

]

---

## Model 3: log $y$ with seasonality - correlogram of residuals
.left-plot[
```{r echo=FALSE, fig.dim=c(6,4.9), out.width="100%"}
augment(lm3_log_s) %>% select(.std.resid) %>% ggAcf() + ggtitle("")
```
]

--

.right-code[

Residuals are still a bit funky.

Still plenty of autocorrelation!

This is not going to be improved by including a quadratic term (try this yourself as an exercise). 

Instead, we can fit an auto-regressive term.

]

---

## Model 4: log $y$ with seasonality and autoregression

This is a rather crude way of fitting this model. Here, we're just trying to demonstrate the concepts. In practice, there are better tools for fitting autoregression and other time series models.

```{r,eval=TRUE}
AP_lag <- AP %>% 
  mutate( log.value = value %>% log) %>%
  mutate( log.value.lag1 = zlag(log.value) ) 

AP_lag

lm4_log_s_AR <- lm(log(value) ~ time + month + log.value.lag1, data = AP_lag)

summary(lm4_log_s_AR) %>% coef %>% round(.,3)
```


---

## Model 4: log $y$ with seasonality and autoregression

.left-code[
```{r eval=TRUE, echo=TRUE, fig.show="hide"}
aug4_log_s_AR_ci <- augment(
  lm4_log_s_AR, 
  dplyr::filter(AP_lag, !is.na(log.value.lag1)),
  interval="confidence")

aug4_log_s_AR_pi <- augment(
  lm4_log_s_AR, 
  dplyr::filter(AP_lag, !is.na(log.value.lag1)),
  interval="prediction")

g4 <- ggplot(aug4_log_s_AR_ci) + 
  aes(x = index, y = value, 
      ymin = exp(.lower), 
      ymax = exp(.upper)) + 
  geom_ribbon(data = aug4_log_s_AR_pi, 
      alpha = .3 ) +
  geom_ribbon(alpha = .2, fill = 12) + 
  geom_line(aes(y = exp(.fitted)), 
            col=4, lwd=1.2) +
  geom_point() + xlab("Time") + 
  ylab("International passengers") 

g4

```
]

.right-plot[
```{r echo=FALSE, fig.dim = c(6,4.5), out.width="100%" }
g4
```

]

---

## Comparison of models 3 and 4

.pull-left[
```{r echo=FALSE, out.width="110%"}
g3_log_s_ci_pi + 
  ggtitle("Model 3: log y ~ time + season")
```

]


.pull-right[
```{r echo=FALSE, out.width="110%"}
g4 + 
  ggtitle("Model 4: log y ~ time + season + ar1")
```

]

---

## Model 4: log $y$ with seasonality and autoregression - correlogram

.left-plot[
```{r echo=FALSE, fig.dim=c(6,4.9), out.width="100%"}
augment(lm4_log_s_AR) %>% select(.std.resid) %>% ggAcf() + ggtitle("")
```
]

--

.right-code[
Bingo!

No more unmodelled autocorrelation.

Remember though, there are more sophisticated methods for modelling time series. 

]


---

## A useful function for time series objects: `decompose()`
.left-code[
```{r echo=TRUE, eval= FALSE}
AirPassengers %>%
  log %>%
  decompose %>% 
  autoplot
```
]

.right-plot[
```{r echo=FALSE, eval= TRUE, fig.dim=c(6,4.9), out.width="100%" }
AirPassengers %>%
  log %>%
  decompose %>% 
  autoplot
```

]

---

## Summary

- Lag plot

- Autoregression and random walk

- Autocorrelation and correlogram

- Air passenger data as an example of model development and residual analysis for time series